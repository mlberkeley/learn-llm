{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny GPT Implementation\n",
    "\n",
    "Welcome to this walkthrough on how to implement GPT from scratch! \n",
    "\n",
    "Much of this notebook is taken from Andrej Karpathy's video [\"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy) and its corresponding resources. However, this notebook is reworked to provide a more notebook-first experience, to aid hands-on learning.\n",
    "\n",
    "This notebook will cover basic concepts such as attention and next-token prediction that are crucial to understanding how GPT works. This walkthrough will not cover much of the finer details of reproducing GPT performance. We'll be using a smaller dataset and single-GPU training.\n",
    "\n",
    "## Dataset\n",
    "Let's download our dataset that we will be training on. GPT-2 and later iterations of GPT were trained on closed-source, large, web-scale datasets. We'll instead be using a much smaller dataset for instructional and practical purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-24 01:49:08--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-12-24 01:49:08 (23.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Let's take a look at our dataset and what it looks like. First, we need to open it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the length, some example text, and the alphabet we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Dataset Length =======\n",
      "Length of the dataset in characters: 1115394\n",
      "======= Sample Text =======\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "======= Alphabet =======\n",
      "Alphabet: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Alphabet Size: 65\n"
     ]
    }
   ],
   "source": [
    "print(\"======= Dataset Length =======\")\n",
    "print(\"Length of the dataset in characters:\", len(text))\n",
    "\n",
    "print(\"======= Sample Text =======\")\n",
    "print(text[:500])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"======= Alphabet =======\")\n",
    "print(\"Alphabet:\", \"\".join(chars))\n",
    "print(\"Alphabet Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our alphabet is consisted of uppercase and lowercase alphabetical characters, in addition to some punctuation and special characters. However, we need to convert these characters into some sort of number representation to feed them into our language model. This process is called **tokenization**. A simple way to transform our characters into tokens is just to take a simple mapping of them, assigning each unique character a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the numbers and create a bidirectional mapping\n",
    "# Start with the string -> integer mapping\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "# Then create the integer -> string mapping\n",
    "itos = { i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an `encode(input: str) -> list[int]` function and a `decode(input: list[int]) -> str` function. `encode` will use our mapping to convert a string to a list of tokens representing the characters, and `decode` will convert a list of tokens to a string representing the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try encoding and decoding a piece of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hello world!\n",
      "==> encoding\n",
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "<== decoding\n",
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "input = \"Hello world!\"\n",
    "print(f\"Input: {input}\")\n",
    "print(\"==> encoding\")\n",
    "encoded = encode(input)\n",
    "print(encoded)\n",
    "print(\"<== decoding\")\n",
    "decoded = decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just created our first encoder and decoder! Normally, tokenization is done with groups of characters rather than single characters like we are doing here. There's a trade off between accuracy and complexity when tokenizing (TODO: describe this in more detail)\n",
    "\n",
    "Now that we have our encoder, we want to use it to preprocess our training data. Let's try encoding the entire dataset using our encoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's wrap it into a torch tensor to allow us to perform efficient matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) <built-in method type of Tensor object at 0x7f9a18612c50>\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(data, dtype=torch.long)\n",
    "print(data.shape, data.type)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data tokenized, let's split it into a training and validation split. Let's use a 90% train/val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "percent_train = 0.9\n",
    "n = int(percent_train*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a model, we want to have some sort of loss that we try to minimize, and something we want to predict. Many large language models today use next-token prediction. Just like the name suggests, at every step in the generation process we want to predict the token that comes next. We can then combine all these tokens to get our final output. \n",
    "\n",
    "Let's take a look at what this means. Let's say we have a `block_size` of 8. This is essentially how large our context window is. For example, when predicting the next token, with a `block_size` of 8, we will not look at any tokens more than 8 tokens behind the one we are trying to predict. In practice, block sizes are much bigger, often in the thousands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single block of 8 tokens is actually providing us a lot of input/output pairs we can use to train our model. For example, we know that `47` is the \"right answer\", or our target when the input is `[18]`. Extending this, `56` is our target when our input is `[18, 47]`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "====\n",
      "Input: tensor([18])\n",
      "Target: 47\n",
      "====\n",
      "Input: tensor([18, 47])\n",
      "Target: 56\n",
      "====\n",
      "Input: tensor([18, 47, 56])\n",
      "Target: 57\n",
      "====\n",
      "Input: tensor([18, 47, 56, 57])\n",
      "Target: 58\n",
      "====\n",
      "Input: tensor([18, 47, 56, 57, 58])\n",
      "Target: 1\n",
      "====\n",
      "Input: tensor([18, 47, 56, 57, 58,  1])\n",
      "Target: 15\n",
      "====\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "Target: 47\n",
      "====\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size+1]\n",
    "y = train_data[1:block_size+1]\n",
    "print(\"Block:\", x)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"====\\nInput: {context}\\nTarget: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the best we can do? We can actually increase our efficiency by utilizing the parallelism provided by GPUs to process multiple blocks at the same time. The number of blocks we process at the same time is called our batch size. Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Target:\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Training:\")\n",
    "print(xb)\n",
    "print(\"Target:\")\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the relation between the Training tensor and the Target tensor? Do you notice any similarities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "Now, let's build out the actual model that will allow us to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instantiate your model and generate 100 text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this produces gibberish. We haven't actually trained our model, so let's do that. We will follow the paper \"Attention is All You Need\" to implement multi-headed self-attention. But first, what is attention?\n",
    "\n",
    "# Attention\n",
    "According to the paper \"Attention is All You Need\", attention is defined as the expression below (more specifically, scaled dot-product attention).\n",
    "$$\\text{Attention}(Q, K ,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "But what does this mean exactly? When we want to predict the next token, we need to find a way to use the tokens that came before it as information. \n",
    "\n",
    "There are many ways to do this with varying complexity, but the simplest way is to take a weighted average of the tokens that came before the current token. For example, we want the 5th token to obtain information (\"pay attention to\") the 4th, 3rd, 2nd, and 1st tokens, but not the 6th or 7th token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this in a for loop for each token, but doing so is very inefficient. Instead, we can use matrices to utilize the parallelism provided by NumPy matrix operations. First, we can devise a matrix that allows each token to only communicate with tokens that appear prior to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this does when multiplied with another matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 7.],\n",
       "        [6., 4.],\n",
       "        [6., 5.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randint(0,10,(3,2)).float()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a @ b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st row of matrix $c$ is equal to the 1st row of matrix $b$, the 2nd row is equal to the average of the 1st and 2nd rows, and the 3rd row is the average of the 1st, 2nd, and 3rd rows. You can verify this for yourself.\n",
    "\n",
    "We can also do this as a batched operation, by extending this into a 3 dimensional array. PyTorch infers extra dimensions automatically, so we don't need to change our weight matrix. \n",
    "\n",
    "For example, if we have a weight matrix with shape $(T, T)$ (representing relative weights in the time dimension), and multiply this with a matrix $A$ with shape $(B, T, C)$ (batch, time, channels), PyTorch will infer the outermost $B$ dimension for our weight matrix. The result will be a $(B, T, T) \\times (B, T, C) \\rightarrow (B, T, C)$ matrix.\n",
    "\n",
    "What a $(B,T,C)$ dimensional matrix actually means in this contex is that we have a $B \\times T$ arrangement of tokens, while the information contained in each token is $C$ dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "x = torch.randn(B,T,C)\n",
    "wei = wei @ x\n",
    "print(x.shape) # we expect this to be (B, T, C) = (4, 8, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also construct this weighted matrix in a variety of different ways. For example, we can use a softmax, which does the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 4\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys, Queries, Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's revisit how we aggregate information. Previously, we took a simple average of all tokens prior to our current token. However, this causes each token to be weighted uniformly equally. In practice, certain tokens are probably more important for certain words than others. For example, in the sentence \"the red dog\", the word \"red\" is probably more important to the word \"dog\" than the word \"the\". As such, we want to be able to model this.\n",
    "\n",
    "A Transformer does this with 3 sets of learnable values. These are called keys, queries, and values. You might have seen these represented as $(K, Q, V)$.\n",
    "\n",
    "When we want to predict a token, we send out a Query, which can be interpreted as a sort of request. We take dot-products of the Query with different Keys, which act as a sort of label for what they contain. The higher this dot-product, the more we pay attention to that word when computing the current token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with single-headed attention first, then move to multi-headed attention. First, we'll create a random matrix with dimensions $(B,T,C)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keys, queries, values are learned values. We can represent them as a simple linear layer. We want this to be a simple linear transform, so we don't want to set a bias term. This will have dimensions $(C, \\text{head size})$. TO DO: explain $\\text{head size}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pass our input data through $K$ and $Q$ individually. Notice that in this stage, they don't communicate yet. This output will have dimensions $(B, T, C) \\times (B, C, \\text{head size}) \\rightarrow (B, C, \\text{head size})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16]) torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "print(k.shape, q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to take the dot product. The efficient way to do this is through a matrix multiply. However, both are $(B, C, \\text{head size})$. We need to transpose one of them before multiplying to get a matrix of shape $(B, C, C)$. These will be our new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = q @ k.transpose(-2, -1)\n",
    "wei.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a problem. Try printing the first row. Current tokens attend to future tokens, which should not happen. As a result, we need to apply the same transform as before to mask future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
      "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
      "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
      "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we want to take a softmax over this to make each row resemble a probability distribution that sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that while the general structure of the matrix is the same, prior tokens on each row are no longer equally weighted, but instead weighted according to the dot-product of their keys with the query.\n",
    "\n",
    "Next, we need to pass our inputs through $V$, or our value matrix. Then, similar to the previous portions, we want to weight our values. In other words, we want to multiply our weights with our values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "v = value(x)\n",
    "out = wei @ v\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few things to notice here.\n",
    "1. This is positionally invariant, at least for now. There is no notion of position. We can rearrange the words in the sentence and it will still work, although our predictions would similarly be misordered.\n",
    "2. Batch dimensions are always independent. There is no communication across different batches.\n",
    "\n",
    "This is also a good time to explain some different concepts:\n",
    "1. The above example is a \"decoder\" block. This is a decoder block because we mask future tokens. \"Encoder\" blocks operate in the same way, but without masking. This is because when encoding we can see future tokens, but when generating (decoding) we can't.\n",
    "2. \"Self-Attention\" means that the keys and values are produced from the same source as queries. Other types of attention such as cross-attention the queries are produced from x but the keys and values come from some other source, such as an encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the basics of self-attention and Transformers, let's put it together and train our own model. Just run this next cell - we've seen everything here before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the Head class, which represents a single head of self-attention. We'll need to fill in 2 methods, `__init__()` and `forward()`.\n",
    "\n",
    "For `__init__()`:\n",
    "- Create a linear layer for key, query, and value\n",
    "- A register buffer for 'tril' is given to you already. A register buffer in PyTorch is a parameter that is not updated during backpropagation, but is included in the model's state_dict. They are commonly used for constants (such as in this case) or for running statistics (that are updated during each iteration). 'tril' in this case serves as a lower triangular mask that is of the correct dimensions\n",
    "- Create a dropout layer\n",
    "\n",
    "For `forward()`:\n",
    "- Pass your input through your linear layer for keys\n",
    "- Pass your input through your linear layer for queries\n",
    "- Compute the dot product between queries and keys\n",
    "- Mask this dot product matrix to make it lower triangular to get a weight matrix\n",
    "- Take a softmax over the rows of the weight matrix\n",
    "- Pass this weight matrix through a dropout layer\n",
    "- Pass the inputs through the value linear layer\n",
    "- Return the weight matrix multiplied by the ouputs of the value linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # (C, head_size) ?? is head_size == C\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # (C, head_size)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # (C, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape \n",
    "        k = self.key(x)   # (B,T,C) -> (B, T, C) @ (C, C)??\n",
    "        q = self.query(x) # (B,T,C) -> (B, T, C) @ (C, C)??\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shang/miniconda3/envs/nanoGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5059\n",
      "step 300: train loss 2.4194, val loss 2.4336\n",
      "step 400: train loss 2.3503, val loss 2.3566\n",
      "step 500: train loss 2.2963, val loss 2.3127\n",
      "step 600: train loss 2.2412, val loss 2.2501\n",
      "step 700: train loss 2.2054, val loss 2.2190\n",
      "step 800: train loss 2.1640, val loss 2.1871\n",
      "step 900: train loss 2.1237, val loss 2.1495\n",
      "step 1000: train loss 2.1031, val loss 2.1304\n",
      "step 1100: train loss 2.0700, val loss 2.1188\n",
      "step 1200: train loss 2.0391, val loss 2.0806\n",
      "step 1300: train loss 2.0258, val loss 2.0650\n",
      "step 1400: train loss 1.9933, val loss 2.0365\n",
      "step 1500: train loss 1.9705, val loss 2.0291\n",
      "step 1600: train loss 1.9639, val loss 2.0485\n",
      "step 1700: train loss 1.9424, val loss 2.0130\n",
      "step 1800: train loss 1.9079, val loss 1.9937\n",
      "step 1900: train loss 1.9081, val loss 1.9875\n",
      "step 2000: train loss 1.8860, val loss 1.9964\n",
      "step 2100: train loss 1.8706, val loss 1.9731\n",
      "step 2200: train loss 1.8571, val loss 1.9594\n",
      "step 2300: train loss 1.8541, val loss 1.9507\n",
      "step 2400: train loss 1.8427, val loss 1.9443\n",
      "step 2500: train loss 1.8153, val loss 1.9422\n",
      "step 2600: train loss 1.8247, val loss 1.9385\n",
      "step 2700: train loss 1.8125, val loss 1.9360\n",
      "step 2800: train loss 1.8047, val loss 1.9231\n",
      "step 2900: train loss 1.8070, val loss 1.9312\n",
      "step 3000: train loss 1.7971, val loss 1.9196\n",
      "step 3100: train loss 1.7665, val loss 1.9131\n",
      "step 3200: train loss 1.7545, val loss 1.9130\n",
      "step 3300: train loss 1.7574, val loss 1.9026\n",
      "step 3400: train loss 1.7560, val loss 1.8963\n",
      "step 3500: train loss 1.7417, val loss 1.8991\n",
      "step 3600: train loss 1.7249, val loss 1.8876\n",
      "step 3700: train loss 1.7273, val loss 1.8804\n",
      "step 3800: train loss 1.7192, val loss 1.8956\n",
      "step 3900: train loss 1.7205, val loss 1.8706\n",
      "step 4000: train loss 1.7101, val loss 1.8560\n",
      "step 4100: train loss 1.7129, val loss 1.8739\n",
      "step 4200: train loss 1.7053, val loss 1.8634\n",
      "step 4300: train loss 1.6999, val loss 1.8461\n",
      "step 4400: train loss 1.7061, val loss 1.8642\n",
      "step 4500: train loss 1.6898, val loss 1.8476\n",
      "step 4600: train loss 1.6876, val loss 1.8338\n",
      "step 4700: train loss 1.6820, val loss 1.8356\n",
      "step 4800: train loss 1.6680, val loss 1.8393\n",
      "step 4900: train loss 1.6700, val loss 1.8363\n",
      "step 4999: train loss 1.6673, val loss 1.8243\n",
      "\n",
      "And they bridle.\n",
      "\n",
      "ShALLO:\n",
      "God madiest bube to take OF thy dagate\n",
      "he art that us his vety crethar ane away, my fears,\n",
      "You have was them of is heart mile dilibouts it ensen mining is the ovets, and the now tilerjess, lesing that this my deam:\n",
      "Theugh are plaw you love.\n",
      "In Bodiet, and whom the sclittle\n",
      "Enout-not what eviles to the our was in\n",
      "him his sout\n",
      "And strume kingnters, If so;\n",
      "Ange the flows yould of which Prive my of.\n",
      "\n",
      "HENRY BOLINGS:\n",
      "You ardsab your great huis courtear teyour-bravence to vow,\n",
      "On parth on, grives you haste?\n",
      " so a cemprease? what the men.\n",
      "\n",
      "CORIOLA:\n",
      "What sween the beggeth of\n",
      "comforty that that lome\n",
      "This she loved the seazented heaving Lady passce of this meen;\n",
      "How what make you fear tals: there longs\n",
      "Tunness, is duke in, more the wanter.\n",
      "\n",
      "HORTENVENCE:\n",
      "Now my To with unlong made to last.\n",
      "\n",
      "PRAMINA:\n",
      "Has I surne desing,\n",
      "He bronain, do thmey severing it:\n",
      "On the shope teasur'd wip.\n",
      "\n",
      "GRINCEMBOR:\n",
      "Which bruke this belike, on so hate and becomer contived:\n",
      "He if now!;\n",
      "To know our her and brunte. fair, fit Henrier.\n",
      "\n",
      "LARY:\n",
      "And him sprive, and now\n",
      "Thou that his well be weep mines so tears\n",
      "Tisfindies to high him a wordn, child friend!\n",
      "\n",
      "BUCKINGInire:\n",
      "Wat the calliants be any in Becanst men,\n",
      "Of infend sir, brothn's my have eath I die;\n",
      "Save thou for Hencher morrulinglow!\n",
      "And forth, not jurst so vow through he it;\n",
      "Than sub love in stide belots men,\n",
      "For they since as beed smet cavonzy to I own,\n",
      "Whave boe; I now, I the depead onm Twreet,\n",
      "Ay, delet! poor so, I have tall wholess thee,\n",
      "sme is justion that druth, tears;\n",
      "River thou a-father my sun.\n",
      "\n",
      "PORALLINA:\n",
      "My flariar straighted wall preing have Lary.\n",
      "\n",
      "BUCKINGHARD\n",
      "WICK:\n",
      "As the roudd. Praties as I, give affites,\n",
      "Thhird bear some to\n",
      "death see tway, her doen: the wife it:\n",
      "The that heave out citt, one the linotly tall wurture.\n",
      "Go, my as balose of at but the peeposer life?\n",
      "\n",
      "COMINIUS:\n",
      "You hove cause; whom you.\n",
      "\n",
      "CORIOLANEY:\n",
      "May, make an nseat, Joance the comforn,\n",
      "Thread patchily brief shown'd fity\n",
      "Hame suppet to his nightrantain\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) # (C, C), learnable linear layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # num_heads, (B, T, C)\n",
    "        out = self.dropout(self.proj(out)) # pass out through linear layer\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUCKINGHAM:\n",
      "3 IVING LEARGARET:\n",
      "Fallst not before you not me, slate ine,\n",
      "And the every to rectinglited is took a well\n",
      "the late of Oponestry?\n",
      "The futh\n",
      "sorroody, you it smile your\n",
      "That heir beast.\n",
      "\n",
      "HORTENMIO:\n",
      "Towe them.\n",
      "On let five thee them that\n",
      "you she Polsmence! and not; must yet go yoursated my need;\n",
      "Our meance a longear tou our his dobe.\n",
      "\n",
      "Clivent Secan they da my sir, the severeed\n",
      "Dear our somet.\n",
      "\n",
      "JOHN MARCETBY:\n",
      "The markly be ast this womere it the your\n",
      "jepreek sleep not teempost; queech seemmen's lie?\n",
      "Down. You hast fult-stand, theirs is the geglory.\n",
      "Is speeks and envy fold be ane wind,\n",
      "It! you 'dreat shall to: fortury.\n",
      "\n",
      "Shalf Reamous.\n",
      "\n",
      "Merry.\n",
      "\n",
      "PAULINA:\n",
      "They forth, doth with weirge, icleive man.\n",
      "\n",
      "FROMVE:\n",
      "Chile! then you you rone kirgn with armiss!\n",
      "Pland, it it sees, why, to\n",
      "And and rove to to father alling\n",
      "Cry a knowly the prace is thunking\n",
      "To to emproved, an you. Yet,y worth\n",
      "down thou out savreht to brumble grave cruntent,\n",
      "Beate you by the ishan but jroy broth,\n",
      "Is I withy worl is the sweace madme\n",
      "And statiages, my loved with quink whiles!\n",
      "Of smul in in all thoughs: at cause:\n",
      "and you, I priage: I if standst?\n",
      "\n",
      "RIVERSBY:\n",
      "Why, Cayied try toich'd ah my one your abour I wome not.\n",
      "\n",
      "PROMPEY:\n",
      "That mele to you whip who you;\n",
      "May you whing that, an hus ancianst,\n",
      "Ands my leept was it the much town the pach you virt:\n",
      "Thou scadion you father, sorth the wronge what's no prame,\n",
      "But Pagchy on two see of him diates becaute, and posins,\n",
      "That can not cage mxplians, Hermand of bush,\n",
      "Stay's thumate thou drealsion to me, orst yound;\n",
      "The cage, and veinged not guards,\n",
      "Or I heaven to masty then thou wearth'd him teever grow:\n",
      "But it behat the fatchiely the his nod,\n",
      "Selbanoy thou aspose'd she way ayds;\n",
      "Thou his though have mine funt, King seekss your ear the said\n",
      "I tenteed strengs, God, I jeoun who,\n",
      "what to it dead! founce!\n",
      "Her vortuned woulder of you\n",
      "That thung dreaves the metter,\n",
      "And thou here thou countion; but twith to the littlein of your call you,\n",
      "And should pment, down, in with thy w\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
